---
title: "Final Report"
author: "Li Liu, Abhishek Pandit, Adam Shelton"
date: "12/7/2019"
header-includes:
    - \usepackage{setspace}\doublespacing
output: 
  pdf_document:
    toc: true
    toc_depth: 1
bibliography: sources.bib
---

```{r main-setup, include=FALSE}
library(tidyverse)
library(here)
library(knitr)
library(kableExtra)
library(skimr)
library(ggcorrplot)
library(treemapify)
library(Cairo)



#set working directory
#setwd('C:/Users/lliu9/Desktop/UML_Project/unsupervised-dating/Final Report')

knitr::opts_chunk$set(echo = FALSE, warning=FALSE, error = FALSE, message = FALSE, dev = "cairo_pdf" , dpi = 400, tidy.opts=list(width.cutoff=50), tidy=TRUE)

# converts a Jupyter notebook to a regular old Python script
flatten_jupyter = function(path_to_notebook) {
  output_path = str_replace(path_to_notebook, ".ipynb", ".py")
  system(paste("python3", "-m", "nbconvert", "--to", "script", paste0("\"", path_to_notebook, "\"")))
  return(output_path)
}
```
# Contributions
```{r contrib, echo=FALSE}
tibble(Liu = c("Item 1", "Item 2", "Item 3"), Pandit = c("Item 1", "Item 2", "Item 3"), Shelton = c("EDA", "AGNES", "DBSCAN")) %>% kable() %>% kable_styling() %>% row_spec(0, bold = TRUE)
```

\newpage
# Introduction

“So tell me about yourself!’’ This seemingly straightforward question in day-to-day interactions is usually met with silence and hesitation. That can no longer be the case for the 1.67 trillion online dating industry, which has grown exponentially in popularity over the last decade. The dating apps, such as OkCupid and Coffee Meets Bagel, are designed to help the singles ‘get to know’ other people for short or long-term romantic relationships. In order to be popular and memorable, users usually have to write a short introduction to advertise themselves. Such activity could be regarded as self-marketing. As the users of dating apps come from diverse backgrounds, we are interested in how users from distinct backgrounds take different approaches to make themselves more memorable. Moreover, we design the framework of scoring users’ self-introduction and algorithm for providing writing tips (such as words for being memorable). Although our project is still preliminary, it has gained a lot of interest among our friends who struggle to find a date online. Also, our methods and analysis have the potential to be adopted by the dating website to improve the users’ experience and better achieve their mission as matchmakers. 

# Literature Review

Self-concept and self-representation have long served as grounds of debate in cognitive and positive psychology [@bruning1999cognitive] as well as social anthropology [@goffman1975presentation]. The recent spread of social networking and its specific affordances have allowed individuals to build different online ‘selves’ [@papacharissi2010networked]. One such critical scenario may be that of mate selection, which several economists and sociologists have likened this to ‘marriage marketplace’ [@hitsch2010matching]. Several online dating service providers in developed countries may facilitate the expansion of potential mates beyond the limits of even extended offline social networks @cacioppo2013marital assert that as many as one in three marriages in the United States is facilitated through these portals. @heino2010relationshopping argue that these avenues further entrench the economic dimension through an acute, implicit awareness of ‘relationshopping’. Herein, potential partners are reduced to entries in a catalog to be scrolled through. In this sense, they suggest an emerging conscientiousness of ‘marketing’, with the product being themselves, and the potential mate assuming the role of a buyer (ibid). This perception thus links the private worlds of romantic intimacy with those of mass consumption and broader perceived appeal to the opposite sex. 

Potentially, we will also use some marketing theories to understand our findings. Selling themselves and finding a mate on OkCupid is not very different from selling a product on eBay. Economists have been interested in the matching problem of demand and supply, such as @hitsch2010matching. Since we do not have data on users’ interactions, we will focus primarily on understanding how people brand themselves to stand out in a crowd. For example, brand awareness is a key metric in marketing to quantify the degree to which people recall or recognize a brand. A high level of brand awareness helps a product stand out and get chosen when consumers face many alternatives. 

This could be applied to understand online dating. Let us imagine your future mate uses the filter to narrow down the consideration sets. He/She might still face many similar choices with high matching scores to choose from. If you want to stand out from the pool, you must make yourself memorable by highlighting the uniqueness. Thus, one possible idea in this project is to explore and understand how users could increase their brand awareness and differentiate themselves in their segments

# Empirical Strategy

# Analysis & Results

## Exploratory Data Analysis

### Descriptive Statistics

```{r descr-stats-demo, echo=FALSE, cache=TRUE}
original_data = read_csv(here("Data", "final_okcupid.csv"))
skim_list = original_data %>% select(-c(dbscan_cluster)) %>% skim() %>% partition()

skim_list$numeric %>% select(-hist) %>% mutate_if(is.numeric, round, digits = 2) %>% kable(caption = "Continuous Variables")
skim_list$character%>% kable(caption = "Other Variables")

original_data %>% mutate_if(is.character, factor) %>% mutate_all(as.numeric) %>% cor(use = "pairwise.complete.obs") %>% ggcorrplot() + labs(title = "Correlation Plot of Demographic Variables")
```

### Visual Analysis

```{r viz-analysis, echo=FALSE}
cat_vars = original_data %>% 
  select(edu, fit, height_group, race_ethnicity) %>% 
  mutate_all(factor) %>% 
  pivot_longer(dplyr::everything()) %>% 
  table() %>% 
  as_tibble() %>% 
  ggplot(aes(area = n, fill = value, label = value)) + 
  geom_treemap() + 
  geom_treemap_text(color = "white", place = "centre", grow = TRUE) + 
  facet_wrap(~ name) + theme(legend.position = "none") + 
  labs(title = "Categorical Variable Distributions")

original_data %>% select(-new_index, -orig_index, -age, -height, -clean_text, -essay9, -dbscan_cluster, -profile_length, -prop_longwords) %>% pivot_longer(-c(flesch, long_words)) %>% ggplot(aes(x = log(flesch + abs(min(flesch)) + 1), y = long_words, color = value)) + geom_point(alpha = 0.3, size = 1) + facet_wrap(~ name) + theme(legend.position = "none") + labs(title = "Flesch score vs. Long words by Variable", x = "Flesch Score (log)", y = "Number of Long Words")
```


### Clusterability

## Clustering of Demographic Data

### K-means

### AGNES

Agglomerative Nesting was used to cluster the demographic data with a bottom-up approach to contrast the K-means clustering. As an AGNES model would not complete on the full data-set, a subset of 2000 observations was used instead.  

## Text Analysis

### Word2Vec

### Topic Modeling

A topic is defined as a mixture of words where each word has a probability of generating from a topic. The example would be words such as 'books', 'college', 'MOOC' could come from the topic of study. A document is a mixture of topics, where a single document can have covered multiple topics. The example would be that a user talks about topics related to career, study, hobbies, and religion in the self-introduction.


### DBSCAN

A DBSCAN model was used to detect outliers within a data-set of 50 vectors calculated by Doc2Vec.

## Combining Text and Demographic Data


### Motivation

So far, we have explored the patterns within the demographic variables using clustering and self-introduction text using LDA. However, in order to quantify whether people of different backgrounds write different topics, we need to model the topic distributions as a function of the metadata (demographic variables and DBScan score). The structural topic modeling (STM) solves the problem as it allows researchers to discover topics and estimate their relationships to document metadata (Roberts et al. 2016). Similar to the Latent Dirichlet Allocation model, STM also assumes there are some latent document-topic and topic-word distributions generating documents. However, STM differs from LDA as it handles the document-associated metadata. As a result, STM allows us to predict how topical prevalence (proportion of a topic across multiple documents) or topic content (the topic composition of terms) would shift when the metadata changes. 

### Estimation

We use the `stm` R package to estimate the model, summarize results, and visualize with the word cloud and topic network. In our project, we are interested in the topical prevalence. So the response ( dependent variable) is the proportion of a topic across multiple self-introductions. The metadata (independent variables) is 'fit', 'education', 'height_group', 'race_ethnicity', and 'dbscan_cluster'. 

In the preparation step, we use `textProcessor` function to stem the words and remove stopwords. Then we use `prepDocuments` function to structure and index the data. As the low-frequency words are probably the more memorable ones, we set the 'lower.thresh' option to 0 to keep all words. 

Then we estimate the model for the topic prevalence using 9 topic models. During the estimation, the proportions of 9 topics are regressed on the metadata. There are many ways to visualize the results. The following plot shows the expected topic proportions 

![Top Topic](Plots/toptopic.jpeg)

The proportions across 9 topics do not have a large gap, as the most common topic is 0.14 and the least common one is 0.10. We observe that the top three topics seem to consist of common nouns and verbs that are not very memorable. So we further explore the most frequent words in the model for both the most common topics (#5, #4, and #7) and the least common topics (#1, #2, and #9). 

![Most Common Topics](Plots/top3topic.jpeg) ![Least Common Topics](Plots/bottom3topic.jpeg)

Although it's hard to tell the quantified difference, we find the words in the least common topics are a little more memorable, such as 'intelligent' and 'dance'. 

When we plot the histograms of topics, we find that most of the topics consist of around 10% of the topics of the documents. Interestingly, the histograms of topics #6 and #8 seem to have a bell-curve shape, which suggests there are many documents have higher or lower proportions of 10%. 

![Most Common Topics](Plots/hist.jpeg) 

In particular, we plot these two plots on the same line. Topic #6 is about 'friend' and 'live' and topic #8 is about 'year' and 'get'. So our interpretation is that topic #6 s about casually looking for friends while topic #8 is about setting a goal of getting something new in the year. As a result, although most users talk about them explicitly, some talk them more and some use other expressions.

![Comparison of Topics 6 and 8](Plots/comparison.jpeg) 

Also, `topicCorr()` calculates correlations between topics, where positive correlations mean that both topics are likely to be discussed within a document.

![Topic Networks](Plots/network.jpeg) 

For the OkCupid beginners, it will be helpful if they could learn what topics are they expected to write together. However, this would only make an ordinary self-introduction. To make it really memorable and stand out, they should consider writing a few topics that are unique and quirky.


### Evaluation

To further understand whether the topics make sense, we use the topicQuality() function to plot the semantic coherence and exclusivity values associated with each topic.

![Topic Quality](Plots/topicQuality.jpeg) 

The results seem to highly correlated with the topic proportions, as topics with higher proportions also have higher semantic coherence and exclusivity values.

### Effect
Finally, we use `estimateEffect()` function to estimating relationships between metadata and topic prevalence. There are 9 regressions under the hood and 12 coefficients associated with each. To effectively analyze the results with 108 coefficients, we compile them into a table by indicating the coefficients which are statistically significant at the level of 5%. These significant coefficients suggest that we can reject the null hypothesis and accept the alternative hypothesis that a relationship exists between the topic prevalence and metadata.

![Significant Coefficients](Plots/Covariates.png) 

We observe from the table that the factor "edu_More than HIgh Schoo" is significant at all of the 9 regressions. This means that being more educated is associated with some variations in the topic proportions. Intuitively, this suggests more educated people might want to write certain topics that make them memorized as intelligent and knowledgable. 

We notice 'height_groupshort' is only significant for regression #7. Topic # 7 has high probability words such as " like, also, think, someone", which seems quite ordinary. It's likely the use of 'also' indicates they are disconfident when introducing themselves, which might because they feel they are too short.

When looking at the coefficients at each regression, we are surprised that in the regression #6, only edu_More than HIgh Schoo" matters for Topic #6's proportions. In comparison, 8 out of the 11 coefficients are significant in the regression #8. Our current understanding is that for topic#8, people with different backgrounds have various ideas of how much to write about it. 

In summary, we get some interesting results during our first time using structural topic modeling. Although we are still learning it, we believe it has great potential to undercover the underlying relationships between metadata and topic prevalence. Such relationships would be useful for OkCupid to provide users with writing tips to make their self-introduction more memorable.




## Output

<img width="30%" src="preference_selection.png"/>
<img width="30%" src="profile_help.png"/>
<img width="30%" src="profile_help_fixed.png"/>

The above three pictures show the protocol of the final product that we have in mind. The protocol uses the example of Li (one of the project members), although the information is hypothetical. The first picture displays his demographic information and a short self-introduction. The second picture shows that the new algorithm highlights the common and memorable words by comparing them with the frequencies among his peers. He also gets a score of 63 and some tips on improving the writing. The third picture is the revised version of previous writing, which now Li has a memorable self-introduction with an increased score of 87. He should be confident to reach out to other users on OkCupid now. 


## Discussion

The project faces several limitations at this stage. Firstly, without data on the outcome (such as the number of messages), we do not know have a relative measure for the self-introductions' memorabilities. As a result, we don't have the outcomes to train the scoring function with machine learning. Secondly, without follow-up interviews, we cannot measure whether the specific choice of words was aimed at authenticity or matches with an awareness of 'relationshopping' (experienced users use certain phrases fraudulently to hook others). Thirdly, without the users' other profile images, we cannot estimate the effect of a memorable self-introduction on outcomes as the quality of images is a potential moderating variable.


# Conclusion

\newpage
# Appendices

## AGNES Code

```{r code = readLines(knitr::purl(here("Clustering", "advanced_clustering.Rmd"), documentation = 1)), echo = T, eval = F}

```

## DBSCAN Code

```{r code = readLines(knitr::purl(here("Clustering", "dbscan_doc2vec.Rmd"), documentation = 1)), echo = T, eval = F}

```

## Doc2Vec Code

```{python code = readLines(flatten_jupyter(here("Vector_Space_Model", "Doc2Vec_Modelling.ipynb"))), echo = T, eval = F}

```


## Structural topic modeling Code

```{r code = readLines(knitr::purl(here("Topic Models", "stm-demo+dbscan.RMD"), documentation = 1)), echo = T, eval = F}

```
\newpage
# References
